{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLS:CNN+cpd.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pims"
      ],
      "metadata": {
        "id": "BS3VFXeo7iwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruptures"
      ],
      "metadata": {
        "id": "SRhtycLPGNGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pims import ImageSequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "import ruptures as rpt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "wbIcGOB_qGtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sSQ4gX287TNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/10changes-Copy1.zip\n",
        "#unzip /content/drive/MyDrive/1_5.zip"
      ],
      "metadata": {
        "id": "DM6izlkV7ihN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/10changes'"
      ],
      "metadata": {
        "id": "R2GNjoRN7ic4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_TDXdcX1HjF"
      },
      "outputs": [],
      "source": [
        "class CPDDatasets:\n",
        "    \"\"\"Class for experiments' datasets.\"\"\"\n",
        "\n",
        "    def __init__(self, experiments_name: str) -> None:\n",
        "        \"\"\"Initialize class.\n",
        "\n",
        "        :param experiments_name: type of experiments (only mnist available now!)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO make \n",
        "        if experiments_name in [\n",
        "            \"mnist\"\n",
        "        ]:\n",
        "            self.experiments_name = experiments_name\n",
        "        else:\n",
        "            raise ValueError(\"Wrong experiment_name {}.\".format(experiments_name))\n",
        "\n",
        "    def get_dataset_(self) -> Tuple[Dataset, Dataset]:\n",
        "        \"\"\"Load experiments' dataset. Only MNIST available.\"\"\"\n",
        "        train_dataset = None\n",
        "        test_dataset = None\n",
        "        path_to_data = path # \"/content/1changes/\"\n",
        "\n",
        "        dataset = MNISTSequenceDataset(path_to_data=path_to_data, type_seq=\"all\")\n",
        "        train_dataset, test_dataset = CPDDatasets.train_test_split_(\n",
        "            dataset, test_size=0.3, shuffle=True\n",
        "        )\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def train_test_split_(\n",
        "        dataset: Dataset, test_size: float = 0.3, shuffle: bool = True\n",
        "    ) -> Tuple[Dataset, Dataset]:\n",
        "        \"\"\"Split dataset on train and test.\n",
        "\n",
        "        :param dataset: dataset for splitting\n",
        "        :param test_size: size of test data\n",
        "        :param shuffle: if True, shuffle data\n",
        "        :return: tuple of\n",
        "            - train dataset\n",
        "            - test dataset\n",
        "        \"\"\"\n",
        "        len_dataset = len(dataset)\n",
        "        idx = np.arange(len_dataset)\n",
        "\n",
        "        if shuffle:\n",
        "            train_idx = random.sample(list(idx), int((1 - test_size) * len_dataset))\n",
        "        else:\n",
        "            train_idx = idx[: -int(test_size * len_dataset)]\n",
        "        test_idx = np.setdiff1d(idx, train_idx)\n",
        "\n",
        "        train_set = Subset(dataset, train_idx)\n",
        "        test_set = Subset(dataset, test_idx)\n",
        "        return train_set, test_set\n",
        "\n",
        "\n",
        "class MNISTSequenceDataset(Dataset):\n",
        "    \"\"\"Class for Dataset consists of sequences of MNIST images.\"\"\"\n",
        "\n",
        "    def __init__(self, path_to_data: str, type_seq: str = \"all\") -> None:\n",
        "        \"\"\"Initialize datasets' parameters.\n",
        "\n",
        "        :param path_to_data: path to folders with MNIST sequences\n",
        "        :param type_seq: type of data for loading (only normal, only anomaly, all)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # set paths to data\n",
        "        self.path_to_data = path_to_data\n",
        "        self.path_to_normal = os.path.join(path_to_data, \"normal/\")\n",
        "        self.path_with_change = os.path.join(path_to_data, \"with_change/\")\n",
        "\n",
        "        self.normal_seq_paths = [\n",
        "            os.path.join(self.path_to_normal, x)\n",
        "            for x in os.listdir(self.path_to_normal)\n",
        "        ]\n",
        "        self.with_change_seq_paths = [\n",
        "            os.path.join(self.path_with_change, x)\n",
        "            for x in os.listdir(self.path_with_change)\n",
        "        ]\n",
        "\n",
        "        # load all sequences, only normal or only sequences with changes\n",
        "        if type_seq == \"all\":\n",
        "            self.sample_paths = self.normal_seq_paths + self.with_change_seq_paths\n",
        "        elif type_seq == \"normal\":\n",
        "            self.sample_paths = self.normal_seq_paths\n",
        "        elif type_seq == \"only_changes\":\n",
        "            self.sample_paths = self.with_change_seq_paths\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'Unknown label type \"{}\". Please, choose one of {{all, normal, only_changes}}.'.format(\n",
        "                    type_seq\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Get datasets' length.\n",
        "\n",
        "        :return: length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.normal_seq_paths) + len(self.with_change_seq_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[np.array, np.array]:\n",
        "        \"\"\"Get one images' sequence and corresponding labels from dataset.\n",
        "\n",
        "        :param idx: index of element in dataset\n",
        "        :return: tuple of\n",
        "             - sequence of images\n",
        "             - sequence of labels\n",
        "        \"\"\"\n",
        "        # read sequences of images\n",
        "        path_img = self.sample_paths[idx]\n",
        "        seq_images = ImageSequence(os.path.join(path_img, \"*_*.png\"))\n",
        "        seq_images = np.transpose(seq_images, (0, 3, 1, 2))[:, 0, :, :].astype(float)#.reshape(64, 1, 28, 28)\n",
        "        ################################################################\n",
        "        \n",
        "        seq_labels = sorted(os.listdir(path_img), key=lambda x: int(x.split(\"_\")[0]))\n",
        "\n",
        "        # get corresponding labels\n",
        "        seq_labels = [int(x.split(\".png\")[0].split(\"_\")[1]) for x in seq_labels]\n",
        "        seq_labels = (np.array(seq_labels) != seq_labels[0]).astype(int)\n",
        "\n",
        "        return seq_images, seq_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_to_gray_(frame: np.array) -> np.array:\n",
        "        \"\"\"Convert PIMS' images to gray scale. In MNIST case, all channels are equals.\n",
        "\n",
        "        :param frame: image\n",
        "        :return: image in gray scale\n",
        "        \"\"\"\n",
        "        return frame[:, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = CPDDatasets(\n",
        "            experiments_name='mnist'\n",
        "        ).get_dataset_()\n"
      ],
      "metadata": {
        "id": "_0BgxmNc1WI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dataset = MNISTSequenceDataset(path_to_data=path, type_seq='all')\n",
        "train_dataset, test_dataset = CPDDatasets.train_test_split_(dataset, test_size=0.3, shuffle=True)"
      ],
      "metadata": {
        "id": "SxOULR6C9AkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader( train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader( test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "P7rNycfOSxJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(10, 8))\n",
        "sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "cols, rows = 8, 8\n",
        "plt.axis(\"off\")\n",
        "\n",
        "img = train_dataset[sample_idx][0]\n",
        "label = train_dataset[sample_idx][1]\n",
        "plt.title(label,  size=10, y=1.1)\n",
        "for i in range(1, cols * rows + 1):\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.axis(\"off\")\n",
        "    label = train_dataset[sample_idx][1][i-1]\n",
        "    plt.title(label)\n",
        "    plt.imshow(img.squeeze()[i-1], cmap=\"gray\")\n",
        "\n",
        "plt.subplots_adjust(wspace=1, hspace=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gymv-DsZ9X7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IFvxnZ0nQ8tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "      super(NeuralNetwork, self).__init__()\n",
        "      self.model_type = 'LinReg'\n",
        "      \n",
        "\n",
        "      self.l1 = nn.Linear(input_size, 1024)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.l2 = nn.Linear(1024, 256)\n",
        "      self.l3 = nn.Linear(256, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "     batch_size, seq_len = x.size()[:2]\n",
        "     if x.type() != 'torch.FloatTensor':\n",
        "            x = x.float() \n",
        "     x = x.flatten(2, -1)\n",
        "     x = x.flatten(1, -1)\n",
        "     out = self.l1(x)\n",
        "     out = self.relu(out)\n",
        "     out = self.l2(out)\n",
        "     out = self.relu(out)\n",
        "     out = self.l3(out)\n",
        "     out = self.sigmoid(out)\n",
        "     return out"
      ],
      "metadata": {
        "id": "do6dD1sUWwSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "class CNN_model(nn.Module):\n",
        "  def __init__(self, in_channels=1, hidden_channels=8, out_channels=16, input_size=28, stride=2, window=8, batch_size=64):\n",
        "      super(CNN_model, self).__init__()\n",
        "      self.model_type = 'LinReg_CNN'\n",
        "      self.window = window\n",
        "      self.batch_size = batch_size\n",
        "      self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,              \n",
        "                out_channels=hidden_channels,            \n",
        "                kernel_size=5,              \n",
        "                stride=2, #1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            #nn.MaxPool2d(kernel_size=1),    \n",
        "        )\n",
        "      self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels, out_channels, 5, \n",
        "                      #1,\n",
        "                      2, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            #nn.MaxPool2d(1),                \n",
        "        )\n",
        "      self.l1 = nn.Linear(int(self.window*out_channels*(input_size/(2*stride))**2), 1024)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.l2 = nn.Linear(1024, 256)\n",
        "      self.l3 = nn.Linear(256, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      print(int(self.window*out_channels*(input_size/(2*stride))**2))\n",
        "  def forward(self, x):\n",
        "     #batch_size, seq_len = x.size()[:2]\n",
        "     if x.type() != 'torch.FloatTensor':\n",
        "            x = x.float() \n",
        "     out = self.conv1(x)\n",
        "     out = self.conv2(out)\n",
        "     \n",
        "     out = out.view(out.size(0)//8, -1) \n",
        "     out = self.l1(out)\n",
        "     out = self.relu(out)\n",
        "     out = self.l2(out)\n",
        "     out = self.relu(out)\n",
        "     out = self.l3(out)\n",
        "     out = self.sigmoid(out)\n",
        "     return out"
      ],
      "metadata": {
        "id": "w0tbgye04YXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader_window:\n",
        "      X, y = batch[:, 0], batch[:, 1]\n",
        "      X, y = np.vstack(X).astype(np.float), np.vstack(y.reshape(len(y), 1)).astype(np.float)\n",
        "     \n",
        "      X, y = torch.tensor(X,  dtype=torch.float).reshape(len(batch)*len(batch[0][0]),1, 28, 28).to(device), torch.tensor(y,  dtype=torch.float).to(device)\n",
        "      break"
      ],
      "metadata": {
        "id": "EJFQuy9f6Kj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "DlHFYmFZ6KqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=8,            \n",
        "                kernel_size=5,              \n",
        "                stride=2,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "        )\n",
        "\n",
        "conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(8, 16, 5, 2, 2),     \n",
        "            nn.ReLU(),                      \n",
        "        )\n",
        "\n",
        "conv3 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 16*2, 5, 2, 2),     \n",
        "            nn.ReLU(),                      \n",
        "        )\n",
        "\n",
        "print(X.shape)\n",
        "out = conv1(X.cpu())\n",
        "print(out.shape)\n",
        "out = conv2(out)\n",
        "print(out.shape)\n",
        "out = conv3(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "b2-VKgKC5IkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = out.view(out.size(0)//8, -1) "
      ],
      "metadata": {
        "id": "_2KkFKCf5I4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "id": "S5l2dc0d7CW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window*32*(28//(2**3))**2"
      ],
      "metadata": {
        "id": "KjcOhuTU72jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gx3EdpzQLjZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_for_logreg(train_data,window=8):\n",
        "    data_x = []\n",
        "    for  i in range(len(train_data)):\n",
        "      #plt.plot(train_data[i][1])\n",
        "      train_data[i][0] = np.array(train_data[i][0])/np.max(train_data[i][0])\n",
        "      cpd = np.argmax(train_data[i][1])\n",
        "      min_ = max(0, cpd- 2*window)\n",
        "      max_ = min(len(train_data[i][0])-window-1,  cpd+1)\n",
        "      for j in range( min_ ,max_):\n",
        "        data_x.append([train_data[i][0][j:j+window], train_data[i][1][j+window]])\n",
        "  \n",
        "    return data_x\n"
      ],
      "metadata": {
        "id": "UqlDArBEq5dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_1(x):\n",
        "  for i in range(len(x)):\n",
        "    if x[i] == 1:\n",
        "      return i\n",
        "  return len(x)"
      ],
      "metadata": {
        "id": "fHOUpGy57Xtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    generator = iter(train_dataloader)\n",
        "    data_window = []\n",
        "    for i in range(len(train_dataloader)):\n",
        "        inputs,labels=next(iter(train_dataloader))\n",
        "        seq_len = len(inputs)\n",
        "        for seq in range(seq_len):\n",
        "          if labels[seq].sum() != 0:\n",
        "            output = my_resnet(torch.tensor(inputs[seq], dtype=torch.float, device=device).reshape(seq_len, 1, 28, 28) ).reshape(64)\n",
        "            for j in range(seq_len - window ):\n",
        "                data_window.append([output[j:j+window], labels[seq][j+window]])\n",
        "            gc.collect()\n",
        "    return data_window\n"
      ],
      "metadata": {
        "id": "d0UDYJoZYuXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_window(train_dataloader, window=8, my_resnet=None, blur=True, do_resnet=False):\n",
        "    generator = iter(train_dataloader)\n",
        "    data_window = []\n",
        "    for i in range(len(train_dataloader)):\n",
        "        inputs,labels=next(iter(train_dataloader))\n",
        "        labels = labels.to(torch.float)\n",
        "        batch_size = len(inputs)\n",
        "        for seq in range(batch_size):\n",
        "          seq_len = len(labels[seq])\n",
        "          if labels[seq].sum() != 0:\n",
        "              cp = find_1(labels[seq])\n",
        "              labels[seq][cp+2:] = 0.\n",
        "              labels[seq][cp-1] = 1.\n",
        "              a = gaussian_filter(labels[seq].to(torch.float), sigma=1.5)\n",
        "              labels[seq] = torch.Tensor(a)\n",
        "              labels[seq][:cp] = 0.\n",
        "              labels[seq][cp] = 1.\n",
        "              labels[seq][cp+window-1:] = 0.\n",
        "              output = inputs[seq].numpy()\n",
        "              min_ = max(0, cp - window - 2)\n",
        "              max_ = min(seq_len-window-1,  cp+1)\n",
        "              for j in range( min_ ,max_):\n",
        "                  data_window.append([output[j:j+window], labels[seq][j+window-1]])\n",
        "        del inputs\n",
        "    data_window = np.array(data_window)\n",
        "    #data_window = np.random.shuffle(data_window)\n",
        "    return data_window\n"
      ],
      "metadata": {
        "id": "qUScE0xyB0WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = get_data_window(train_dataloader, my_resnet=my_resnet, blur=True)\n",
        "#train_data = np.array(train_data)\n",
        "#np.save('/content/drive/MyDrive/train_data_window.npy', train_data)"
      ],
      "metadata": {
        "id": "uPWiQTFVvRKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_data =  get_data_window(test_dataloader, my_resnet=my_resnet, blur=True)\n",
        "#test_data = np.array(test_data)\n",
        "#np.save('/content/drive/MyDrive/test_data_window.npy', test_data)"
      ],
      "metadata": {
        "id": "_NoWBYkCvW_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data_resnet = np.load('/content/drive/MyDrive/train_data_window.npy', allow_pickle=True)\n",
        "#test_data_resnet = np.load('/content/drive/MyDrive/test_data_window.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "0VCX5qZ7yZrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MNISTSequenceDataset(path_to_data=path, type_seq='all')\n",
        "train_dataset, test_dataset = CPDDatasets.train_test_split_(dataset, test_size=0.3, shuffle=True)"
      ],
      "metadata": {
        "id": "vYbfbdlOb-Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader( train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader( test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "3lqD-2GKb5o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_window =  get_data_window(test_dataloader, blur=True)\n",
        "train_data_window =  get_data_window(train_dataloader, blur=True)\n",
        "np.random.shuffle(train_data_window)\n",
        "np.random.shuffle(test_data_window)\n"
      ],
      "metadata": {
        "id": "chFxcrnfyisU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "orgHrxNbes_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "outer = gridspec.GridSpec(9, 1, wspace=0.5, hspace=0.5)\n",
        "sample_idx = 2+7+10\n",
        "for i in range(9):\n",
        "    inner = gridspec.GridSpecFromSubplotSpec(1, 8,\n",
        "                    subplot_spec=outer[i], wspace=0.5, hspace=0.5)\n",
        "    label = train_data_window[sample_idx+i][1]#[j+window]\n",
        "    img = train_data_window[sample_idx+i][0]#[j:j+window]\n",
        "    ax1 = plt.Subplot(fig, outer[i])\n",
        "    ax1.set_title(label)\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    fig.add_subplot(ax1)\n",
        "    for j in range(8):\n",
        "        ax = plt.Subplot(fig, inner[j])\n",
        "        #t = ax.text(0.5,0.5, 'outer=%d, inner=%d' % (i, j))\n",
        "        ax.imshow(img[j], cmap=\"gray\")\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        fig.add_subplot(ax)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5xyaKz1pS5N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_D08oKAXS5SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class0 = 0\n",
        "class1 = 0\n",
        "for d in train_data_window:\n",
        "  if d[1] < 0.1:\n",
        "    class0 += 1\n",
        "  else:\n",
        "    class1 += 1\n",
        "class0, class1, class0 + class1, len(train_data_window)"
      ],
      "metadata": {
        "id": "nsbIWC0N3pKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(data, batch_size=64, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    \n",
        "    for idx in range(0, len(data), batch_size):\n",
        "        # We make sure we dont get the last bit if its not batch_size size\n",
        "        if idx + batch_size < len(data):\n",
        "            # Here you would need to get the max length of the batch,\n",
        "            # and normalize the length with the PAD token.\n",
        "            if padding:\n",
        "                max_batch_length = 0\n",
        "\n",
        "                # Get longest sentence in batch\n",
        "                for seq in data[idx : idx + batch_size]:\n",
        "                    if len(seq) > max_batch_length:\n",
        "                        max_batch_length = len(seq)\n",
        "\n",
        "                # Append X padding tokens until it reaches the max length\n",
        "                for seq_idx in range(batch_size):\n",
        "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
        "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
        "            \n",
        "            batches.append(np.array(data[idx : idx + batch_size]))\n",
        "    #print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "iQ2vDq2ZuEuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        X, y = batch[:, 0], batch[:, 1]\n",
        "        X, y = np.vstack(X).astype(np.float), np.vstack(y.reshape(len(y), 1)).astype(np.float)\n",
        "        if model.model_type == 'LinReg':\n",
        "              X, y = torch.tensor(X,  dtype=torch.float).reshape(len(batch), len(batch[0][0]), 28, 28).to(device), torch.tensor(y,  dtype=torch.float).to(device)\n",
        "        if model.model_type == 'LinReg_CNN':\n",
        "              X, y = torch.tensor(X,  dtype=torch.float).reshape(len(batch)*len(batch[0][0]),1, 28, 28).to(device), torch.tensor(y,  dtype=torch.float).to(device)        \n",
        "        pred = model(X)\n",
        "        #weight = []\n",
        "        #for k in range(len(y)):\n",
        "        #      if y[k] < 0.5:\n",
        "        #        weight.append( 0.1 )\n",
        "        #      else:\n",
        "        #        weight.append( 1. )\n",
        "        #weight = torch.Tensor(weight).to(torch.float)\n",
        "        #loss = weighted_mse_loss(pred, y, weight)\n",
        "        loss = loss_fn(pred, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    \n",
        "        total_loss += loss.detach().item()\n",
        "        \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = np.vstack(X).astype(np.float), np.vstack(y.reshape(len(y), 1)).astype(np.float)\n",
        "            if model.model_type == 'LinReg':\n",
        "                #X, y = np.vstack(X).astype(np.float), np.vstack(y.reshape(len(y), 1)).astype(np.float)\n",
        "                X, y = torch.tensor(X,  dtype=torch.float).reshape(len(batch), len(batch[0][0]), 28, 28).to(device), torch.tensor(y,  dtype=torch.float).to(device)\n",
        "            if model.model_type == 'LinReg_CNN':\n",
        "                X, y = torch.tensor(X,  dtype=torch.float).reshape(len(batch)*len(batch[0][0]),1, 28, 28).to(device), torch.tensor(y,  dtype=torch.float).to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            total_loss += loss.detach().item()\n",
        "        \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
        "\n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "    \n",
        "    #print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        #clear_output(wait=True)\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "        \n",
        "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "        train_loss_list += [train_loss]\n",
        "        \n",
        "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "        \n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print()\n",
        "        \n",
        "    return train_loss_list, validation_loss_list\n",
        " "
      ],
      "metadata": {
        "id": "Wd9dyE9JstNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bound(x, tr=0.8, window=8):\n",
        "  start = 0\n",
        "  end = 0\n",
        "  for i in range(1, len(x)):\n",
        "    if x[i] > tr and x[i-1] <= tr:\n",
        "      start = i\n",
        "    if x[i] < tr and x[i-1] >= tr:\n",
        "      end = i\n",
        "  return [start, end]"
      ],
      "metadata": {
        "id": "vECFnjrEusLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_window = batchify_data(train_data_window)\n",
        "val_dataloader_window = batchify_data(test_data_window)"
      ],
      "metadata": {
        "id": "-0tDLiQruyaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_mse_loss(input, target, weight):\n",
        "    return torch.sum(weight * (input - target) ** 2).mean()"
      ],
      "metadata": {
        "id": "Bb6ZYXg4Of11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "Dlh0Iilkclow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cTbprti9jb0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "class CNN_model(nn.Module):\n",
        "  def __init__(self, in_channels=1, hidden_channels=8, out_channels=16, input_size=28, stride=2, window=8, batch_size=64):\n",
        "      super(CNN_model, self).__init__()\n",
        "      self.model_type = 'LinReg_CNN'\n",
        "      self.window = window\n",
        "      self.batch_size = batch_size\n",
        "      self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,              \n",
        "                out_channels=hidden_channels,            \n",
        "                kernel_size=5,              \n",
        "                stride=2, #1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "        )\n",
        "      self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(hidden_channels, out_channels, 5, \n",
        "                      #1,\n",
        "                      2, 2),     \n",
        "            nn.ReLU(),                      \n",
        "        )\n",
        "      self.conv3 = nn.Sequential(         \n",
        "            nn.Conv2d(out_channels, 2*out_channels, 5, \n",
        "                      #1,\n",
        "                      2, 2),     \n",
        "            nn.ReLU(),                      \n",
        "        )\n",
        "      \n",
        "      self.l1 = nn.Linear(\n",
        "          2*4096, #int(self.window*out_channels*(input_size/(2*stride))**2)\n",
        "          1024)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.l2 = nn.Linear(1024, 256)\n",
        "      self.l3 = nn.Linear(256, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      print(int(self.window*out_channels*(input_size/(2*stride))**2))\n",
        "  \n",
        "  def forward(self, x):\n",
        "     #batch_size, seq_len = x.size()[:2]\n",
        "     if x.type() != 'torch.FloatTensor':\n",
        "            x = x.float() \n",
        "     out = self.conv1(x)\n",
        "     out = self.conv2(out)\n",
        "     out = self.conv3(out)\n",
        "     \n",
        "     out = out.view(out.size(0)//8, -1) \n",
        "     out = self.l1(out)\n",
        "     out = self.relu(out)\n",
        "     out = self.l2(out)\n",
        "     out = self.relu(out)\n",
        "     out = self.l3(out)\n",
        "     out = self.sigmoid(out)\n",
        "     return out"
      ],
      "metadata": {
        "id": "IbWLYlRWXCDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window = 8\n",
        "model = CNN_model(in_channels=1, hidden_channels=16, out_channels=32, window=window).to(device)\n",
        "\n",
        "#model = NeuralNetwork(window*28*28).to(device)\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.001)#\n",
        "#opt = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
        "#opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "loss_fn = nn.MSELoss() #    nn.CrossEntropyLoss() # \n"
      ],
      "metadata": {
        "id": "Xa3rmONp7A_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader_window, val_dataloader_window, 50)"
      ],
      "metadata": {
        "id": "t2__RZ5C7BLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(validation_loss_list, label='val')\n",
        "plt.plot(train_loss_list, label='train')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ZMcTzzuD7Qxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "fTo7sSs2D11n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_data_exaples(test_dataloader, n=10, no_cpd=3, seq_len=64, window=8,     blur = True):\n",
        "    data_examples = []\n",
        "    inputs_examples = []\n",
        "    gen = iter(test_dataloader)\n",
        "    flag = 0\n",
        "    flag64 = 0\n",
        "    while len(data_examples) != n:\n",
        "        inputs,labels=next(gen)\n",
        "        #\n",
        "        labels = labels.to(torch.float)\n",
        "        batch_size = len(inputs)\n",
        "        \n",
        "        for seq in range( batch_size - 1):\n",
        "          data_example = []\n",
        "          seq_len = len(labels[seq])\n",
        "          cp = find_1(labels[seq])\n",
        "          if cp != seq_len and cp > window:\n",
        "            labels[seq][cp+2:] = 0.\n",
        "            labels[seq][cp-1] = 1.\n",
        "            a = gaussian_filter(labels[seq].to(torch.float), sigma=1.5)\n",
        "            labels[seq] = torch.Tensor(a)\n",
        "            labels[seq][:cp] = 0.\n",
        "            labels[seq][cp] = 1.\n",
        "            labels[seq][cp+window-1:] = 0.\n",
        "            inputs_seq = inputs[seq].numpy()\n",
        "            inputs_examples.append(inputs_seq)\n",
        "            gc.collect()\n",
        "            flag += 1\n",
        "            for j in range( 0 , seq_len-window):\n",
        "                  data_example.append([inputs_seq[j:j+window], labels[seq][j+window-1]])\n",
        "            data_examples.append(data_example)\n",
        "          if cp == seq_len and flag64 < no_cpd:\n",
        "            flag64 += 1\n",
        "            flag += 1\n",
        "            inputs_seq = inputs[seq].numpy()\n",
        "            inputs_examples.append(inputs_seq)\n",
        "            for j in range( 0 , seq_len-window):\n",
        "                  data_example.append([inputs_seq[j:j+window], labels[seq][j+window-1]])\n",
        "            data_examples.append(data_example)\n",
        "\n",
        "          if flag == n:\n",
        "            break\n",
        "        \n",
        "    return   data_examples, inputs_examples"
      ],
      "metadata": {
        "id": "N77f3DUV7Ugz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_examples, inputs_examples = get_n_data_exaples(test_dataloader)"
      ],
      "metadata": {
        "id": "TnrlcM5X6R3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(len(data_examples), figsize=(15,10))\n",
        "k = -1\n",
        "for data_example in data_examples:\n",
        "    k += 1\n",
        "    pred = []\n",
        "    for j in range( len(data_example)):\n",
        "        X = data_example[j][0]\n",
        "        if model.model_type == 'LinReg_CNN':\n",
        "          X = torch.tensor(X,  dtype=torch.float).reshape( X.shape[0], 1, X.shape[1], X.shape[2]).to(device)\n",
        "        else:\n",
        "          X = torch.tensor(X,  dtype=torch.float).reshape(1,   X.shape[0], X.shape[1], X.shape[2]).to(device)\n",
        "        pred.append(model( X ).detach().cpu().numpy() )\n",
        "    pred = np.array(pred)\n",
        "    pred = pred.reshape(len(pred))\n",
        "\n",
        "    true_labels = [data_example[i][1] for i in range(len(data_example))]\n",
        "    ax[k].plot(pred, label = 'pred')\n",
        "    ax[k].plot(true_labels, label='true')\n",
        "    ax[k].legend()\n",
        "\n",
        "    real = get_bound(true_labels, tr = 0.985)\n",
        "    preds = get_bound(pred, tr = 0.5)\n",
        "    input = inputs_examples[k]\n",
        "    #rpt.display(output[window:], [real[0],  seq_len], [preds[0],  seq_len])"
      ],
      "metadata": {
        "id": "_kRmOg1Hzo1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(15, 8))\n",
        "cols, rows = 8, 8\n",
        "plt.axis(\"off\")\n",
        "\n",
        "img = inputs_examples[-1][window:]\n",
        "label = true_labels[-1]\n",
        "labels_blur = [data_example[i][1] for i in range(len(data_example))]\n",
        "\n",
        "#plt.title(label,  size=10, y=1.1)\n",
        "for i in range(1, len(img) ):\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.axis(\"off\")\n",
        "    label = np.round(labels_blur[i].numpy(), 2, )\n",
        "    plt.title('t '+str(label) + ', p ' + str(round(pred[i], 2))) \n",
        "    plt.imshow(img.squeeze()[i-1], cmap=\"gray\")\n",
        "\n",
        "plt.subplots_adjust(wspace=1, hspace=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cBt70lpL8pQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_examples, inputs_examples  = get_n_data_exaples(test_dataloader, n=1000, no_cpd=500)"
      ],
      "metadata": {
        "id": "Aq_vUwj52ai4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_examples)"
      ],
      "metadata": {
        "id": "w3dV2v6a4yhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recall(data_examples, model, threshold = 0.3):\n",
        "    k = -1\n",
        "    recalls = []\n",
        "    for data_example in data_examples:\n",
        "        k += 1\n",
        "        pred = []\n",
        "        for j in range( len(data_example)):\n",
        "            X = data_example[j][0]\n",
        "            if model.model_type == 'LinReg_CNN':\n",
        "              X = torch.tensor(X,  dtype=torch.float).reshape( X.shape[0], 1, X.shape[1], X.shape[2]).to(device)\n",
        "            else:\n",
        "              X = torch.tensor(X,  dtype=torch.float).reshape(1,   X.shape[0], X.shape[1], X.shape[2]).to(device)\n",
        "            pred.append(model( X ).detach().cpu().numpy() )\n",
        "        pred = np.array(pred).reshape(len(pred))\n",
        "        true_labels = np.array([data_example[i][1] for i in range(len(data_example))])\n",
        "        \n",
        "        # recall = TP / Total Actual Positive\n",
        "        total_p = np.array([true_labels > threshold]).sum()\n",
        "        if total_p != 0:\n",
        "            recall =  (np.array([pred > threshold])*np.array([true_labels > threshold ])).sum() / total_p\n",
        "            recalls.append(recall)\n",
        "    #print(recalls)\n",
        "    return np.mean(recalls)"
      ],
      "metadata": {
        "id": "2X4M6j0p4y-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_width(data_examples, model, threshold = 0.3):\n",
        "    k = -1\n",
        "    pred_widths = []\n",
        "    for data_example in data_examples:\n",
        "        k += 1\n",
        "        pred = []\n",
        "        for j in range( len(data_example)):\n",
        "            X = data_example[j][0]\n",
        "            if model.model_type == 'LinReg_CNN':\n",
        "              X = torch.tensor(X,  dtype=torch.float).reshape( X.shape[0], 1, X.shape[1], X.shape[2]).to(device)\n",
        "            else:\n",
        "              X = torch.tensor(X,  dtype=torch.float).reshape(1,   X.shape[0], X.shape[1], X.shape[2]).to(device)\n",
        "            pred.append(model( X ).detach().cpu().numpy() )\n",
        "        pred = np.array(pred).reshape(len(pred))\n",
        "        pred_width = np.array([pred > threshold]).sum()\n",
        "        true_labels = np.array([data_example[i][1] for i in range(len(data_example))])\n",
        "        total_p = np.array([true_labels > threshold]).sum()\n",
        "        if total_p != 0:\n",
        "            pred_widths.append(pred_width)\n",
        "    #print(pred_widths)\n",
        "    return np.mean(pred_widths)"
      ],
      "metadata": {
        "id": "sw_x_NxddjRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metric(test_dataloader, model,  n=200, no_cpd=100, threshold=0.1 ):\n",
        "    data_examples, inputs_examples = get_n_data_exaples(test_dataloader)\n",
        "    fig, ax = plt.subplots(len(data_examples), figsize=(15,10))\n",
        "    k = -1\n",
        "    for data_example in data_examples:\n",
        "        k += 1\n",
        "        pred = []\n",
        "        for j in range( len(data_example)):\n",
        "            X = data_example[j][0]\n",
        "            if model.model_type == 'LinReg_CNN':\n",
        "              X = torch.tensor(X,  dtype=torch.float).reshape( X.shape[0], 1, X.shape[1], X.shape[2]).to(device)\n",
        "            else:\n",
        "              X = torch.tensor(X,  dtype=torch.float).reshape(1,   X.shape[0], X.shape[1], X.shape[2]).to(device)\n",
        "            pred.append(model( X ).detach().cpu().numpy() )\n",
        "        pred = np.array(pred)\n",
        "        pred = pred.reshape(len(pred))\n",
        "\n",
        "        true_labels = [data_example[i][1] for i in range(len(data_example))]\n",
        "        ax[k].plot(pred, label = 'pred')\n",
        "        ax[k].plot(true_labels, label='true')\n",
        "        ax[k].legend()\n",
        "\n",
        "        real = get_bound(true_labels, tr = 0.985)\n",
        "        preds = get_bound(pred, tr = 0.5)\n",
        "        input = inputs_examples[k]\n",
        "    plt.show()\n",
        "\n",
        "    data_examples, inputs_examples = get_n_data_exaples(test_dataloader, n, no_cpd)\n",
        "    recall = get_recall(data_examples, model, threshold)\n",
        "    pred_width = get_pred_width(data_examples, model, threshold)\n",
        "    return recall, pred_width"
      ],
      "metadata": {
        "id": "BQrqpJlofVfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall, pred_width = get_metric(test_dataloader, model)\n",
        "recall, pred_width"
      ],
      "metadata": {
        "id": "qDb6aHie-zIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GOaM0-nDs_iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save( model, '/content/drive/MyDrive/colab_anna/DLS/model10.pt—Ä')"
      ],
      "metadata": {
        "id": "DtjdCTfLs_oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H-IYT_QN_sdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}